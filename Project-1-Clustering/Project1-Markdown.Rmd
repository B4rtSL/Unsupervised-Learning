---
title: "Project1-Markdown"
output: html_document
date: "2023-12-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# NBA Statistics data set - Clustering.

## Introduction.

::: {style="text-align: justify"}
The NBA basketball league is famous for its players unique level of skill. Those players, who are good enough to play in the league are selected carefully by the scouts, with the statistics of their performance being constantly analysed.

The goal of the project is to get as many valuable insights into the data set as possible with the clustering technique, which might help with assessing players on-court value. Although clustering, as a part of the unsupervised learning field, is used to identify the possible groups in the data in instances where labels for such groups are not provided, and in case of this data set there in fact is some form of labeling (court roles are assigned), the aim is to provide the information based on the value which players bring to the court rather than simple assignment of the on-court position.

I would like to conduct an analysis which might divide players into clusters based on their statistics inside of the on-court role groups. I would also like to check if some players will fall out of the clusters and if so to also try to find an answer why.
:::

## Review of existing solutions.

::: {style="text-align: justify"}
While doing the research of currently existing solutions for this project I came across some interesting analyses, but they are mostly focused on obtaining hidden skills and tendencies of players.

For example, the author of "Generating NBA Archetypes Using K-MeansÂ Clustering" article (<https://bestballstats.com/2022/07/23/generating-nba-archetypes-using-k-means-clustering/>) is separating the defensive and offensive roles on the court and assigning the archetypes in each of those categories with no regards to the classical on-court positioning, focusing only on the statistics which define the tendencies on the court. The archetypes themselves are created based on k-means clustering by the chosen statistics and named after analysis of the clusters contents. Such approach might be helpful when trying to find a player with a certain play-style on the given side of the court.

In another interesting article (<https://towardsdatascience.com/redefining-nba-player-classifications-using-clustering-36a348fa54a8>) the author uses the PCA dimension reduction technique to gather information about the players from the wide range of the statistics. After the reduction, author describes "ingredients" that contribute the most to creation of every Principal Components and which on court tendencies each PC reflects. On top of that hierarchical clustering is conducted, grouping the players based on their profiles, which are developed based on the PC "ingredients".

Taking into consideration the focus of above analyses on players tendencies and on-court role archetypes I believe that there exists some space for conducting the analysis focusing on the on-court impact itself and that combining the results of archetype-focused analysis with impact-focused analysis might bring valuable insights into teams rosters.
:::

## Data set review and preparation.

```{r}
data <- read.csv('nba_2022-23_all_stats_with_salary.csv')
```

```{r}
data$Position = as.factor(data$Position)
summary(data$Position)
data[data$Position == 'PG-SG' | data$Position == 'SF-PF' | data$Position == 'SF-SG' | data$Position == 'SG-PG',]
```

::: {style="text-align: justify"}
There are 7 players that are assigned with a mix of a classically considered basketball court positions. In order to achieve coherent position labeling, I have decided to manually assign positions (PG, SG, SF, PF, C) to those players taking into consideration either their historical roles or their role in lineups for the season.
:::

```{r}
data$Position = as.character(data$Position)

# Kyrie Irving - PG
data[data$Player.Name == 'Kyrie Irving',]['Position'] = 'PG'
# Mikal Bridges
data[data$Player.Name == 'Mikal Bridges',]['Position'] = 'SF'
# Spencer Dinwiddie
data[data$Player.Name == 'Spencer Dinwiddie',]['Position'] = 'PG'
# Patrick Beverley
data[data$Player.Name == 'Patrick Beverley',]['Position'] = 'PG'
# Matisse Thybulle
data[data$Player.Name == 'Matisse Thybulle',]['Position'] = 'SF'
# George Hill
data[data$Player.Name == 'George Hill',]['Position'] = 'PG'
# Kevin Knox
data[data$Player.Name == 'Kevin Knox',]['Position'] = 'SF'

data$Position = as.factor(data$Position)
summary(data$Position)
```

::: {style="text-align: justify"}
Next step is to select statistics, based on which the clustering will be conducted. Chosen stats quite well reflect players influence on the team performance. There is only one row which has 'NA' values, therefore it is removed from the data set - the player did not attempt any shots during his time on court. Furthermore during analysis of the data set it turned out, that Nikola Jokic, Finals MVP from season 2022-2023, is not included, so he was added manually, as player with such significant impact on the team would for sure stand as a good reference point for further analysis.
:::

```{r}
shorter_stats <- data[c('Player.Name', 'Position', 'MP', 'PTS', 'AST', 'STL', 'BLK', 'TRB', 'eFG.', 'TS.', 'USG.', 'WS.48','VORP')]

shorter_stats[!complete.cases(shorter_stats),]

shorter_stats = shorter_stats[complete.cases(shorter_stats),]

nikola_jokic = c('Nikola Jokic', 'C', 33.7, 24.5, 9.8, 1.3, 0.7, 11.8, 0.66, 0.701, 27.2, 0.308, 8.8)

shorter_stats <- rbind(shorter_stats, nikola_jokic)

numeric_cols <- c('MP', 'PTS', 'AST', 'STL', 'BLK', 'TRB', 'eFG.', 'TS.', 'USG.', 'WS.48','VORP')

shorter_stats[numeric_cols] <- lapply(shorter_stats[numeric_cols], as.numeric)

columns_shortcut = c('Player.Name','MP', 'PTS', 'AST', 'STL', 'BLK', 'TRB', 'eFG.', 'TS.', 'USG.', 'WS.48','VORP')
```

::: {style="text-align: justify"}
The data is divided to 5 sub-groups based on the classical on-court role. Statistical summary of characteristics for each group may be found below.

In order to see the whole table in correct size and quality please click the "Show in the new window" button. Unfortunately the number of chosen stats exceeds the Markdown's ability to display them conveniently.

The statistics themselves reflect:

MP - Minutes played per game.

PTS - Points per game.

AST - Assists per game.

STL - Steals per game.

BLK - Blocks per game.

TRB - Total rebounds per game (offensive + defensive rebounds).

eFG. - Effective Field Goal Percentage (shooting statistic adjusted for worth of 3-point and 2-point shots).

TS. - True Shooting Percentage - measure of shooting efficiency that takes into account field goals, 3-point field goals, and free throws.

USG. - Usage Percentage - estimate of the percentage of team plays used by a player while he was on the floor.

WS.48 - Win Shares Per 48 Minutes - an estimate of the number of wins contributed by the player per 48 minutes.

VORP - Value Over Replacement Player - a player's collective impact on their team, in contrast to that of an average player who could replace them in the same position.
:::

```{r}
stats_PG = shorter_stats[shorter_stats$Position == 'PG', columns_shortcut]
summary(stats_PG[,2:12])
```

```{r, echo=TRUE}

stats_SG = shorter_stats[shorter_stats$Position == 'SG', columns_shortcut]
summary(stats_SG[,2:12])
```

```{r}
stats_SF = shorter_stats[shorter_stats$Position == 'SF', columns_shortcut]
summary(stats_SF[,2:12])
```

```{r}
stats_PF = shorter_stats[shorter_stats$Position == 'PF', columns_shortcut]
summary(stats_PF[,2:12])

```

```{r}
stats_C = shorter_stats[shorter_stats$Position == 'C', columns_shortcut]
summary(stats_C[,2:12])
```

::: {style="text-align: justify"}
Correlation matrices for statistics from each group of court roles show positive relations between the statistics, therefore, knowing that they are used to measure players impact on the field one can assume that chosen statistics would reflect players positive or negative contribution towards teams' performance.
:::

```{r}
library(gridExtra)
library(corrplot)

corrplot(cor(stats_PG[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats PG", mar=c(0,0,2,0))

corrplot(cor(stats_SG[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats SG", mar=c(0,0,2,0))

corrplot(cor(stats_SF[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats SF", mar=c(0,0,2,0))

corrplot(cor(stats_PF[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats PF", mar=c(0,0,2,0))

corrplot(cor(stats_C[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats C", mar=c(0,0,2,0))
```

The data is then standardized with the z-score method.

```{r}
scaled_PG = as.data.frame(lapply(stats_PG[,2:12], scale))

scaled_SG = as.data.frame(lapply(stats_SG[,2:12], scale))

scaled_SF = as.data.frame(lapply(stats_SF[,2:12], scale))

scaled_PF = as.data.frame(lapply(stats_PF[,2:12], scale))

scaled_C = as.data.frame(lapply(stats_C[,2:12], scale))
```

## Clustering tendency.

::: {style="text-align: justify"}
The data clustering tendency is checked for each court-role. The Hopkins stat is either close to 0.7 or above for each group, therefore significant clusters might exist.
:::

```{r}
library(factoextra)
library(cluster)

get_clust_tendency(scaled_PG, 2, graph=T)
get_clust_tendency(scaled_SG, 2, graph=T)
get_clust_tendency(scaled_SF, 2, graph=T)
get_clust_tendency(scaled_PF, 2, graph=T)
get_clust_tendency(scaled_C, 2, graph=T)
```

## Distance-based clustering.

### Optimal number of clusters for each group based on court role.

::: {style="text-align: justify"}
Knowing that the data could be significantly clustered we continue the analysis with checking how well the data could be clustered with the k-means and PAM clustering methods and what is more important for how many clusters it would work the best.

The key difference between these two methods is that k-means is based on the idea of centroids, while PAM is based on the idea of medoids.

The CLARA method has no use in this case, as the data set is relatively small - PAM algorithm should be good enough (as they both work based on medoids concept).
:::

```{r}
PG_viz_kmeans = fviz_nbclust(scaled_PG, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans PG')

SG_viz_kmeans = fviz_nbclust(scaled_SG, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans SG')

SF_viz_kmeans = fviz_nbclust(scaled_SF, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans SF')

PF_viz_kmeans = fviz_nbclust(scaled_PF, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans PF')

C_viz_kmeans = fviz_nbclust(scaled_C, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans C')
```

```{r}
PG_viz_pam = fviz_nbclust(scaled_PG, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM PG')

SG_viz_pam = fviz_nbclust(scaled_SG, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM SG')

SF_viz_pam = fviz_nbclust(scaled_SF, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM SF')

PF_viz_pam = fviz_nbclust(scaled_PF, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM PF')

C_viz_pam = fviz_nbclust(scaled_C, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM C')
```

```{r}
library(gridExtra)

grid.arrange(PG_viz_kmeans, PG_viz_pam, ncol=2)
grid.arrange(SG_viz_kmeans, SG_viz_pam, ncol=2)
grid.arrange(SF_viz_kmeans, SF_viz_pam, ncol=2)
grid.arrange(PF_viz_kmeans, PF_viz_pam, ncol=2)
grid.arrange(C_viz_kmeans, C_viz_pam, ncol=2)
```

::: {style="text-align: justify"}
The results are consistent in terms of number of clusters - the best silhouette values are returned when 2 clusters are selected.
:::

### K-means clustering.

```{r}
PG_clust <- eclust(scaled_PG, FUNcluster = 'kmeans', k=2, graph=F)

SG_clust <- eclust(scaled_SG, FUNcluster = 'kmeans', k=2, graph=F)

SF_clust <- eclust(scaled_SF, FUNcluster = 'kmeans', k=2, graph=F)

PF_clust <- eclust(scaled_PF, FUNcluster = 'kmeans', k=2, graph=F)

C_clust <- eclust(scaled_C, FUNcluster = 'kmeans', k=2, graph=F)
```

```{r}
grid.arrange(
  fviz_cluster(PG_clust, 
               data = scaled_PG, 
               elipse.type = 'convex', 
               main='Kmeans PG',
               show_labels = FALSE),
  fviz_silhouette(PG_clust),
  ncol=2)

grid.arrange(
  fviz_cluster(SG_clust, 
               data = scaled_SG, 
               elipse.type = 'convex', 
               main='Kmeans SG'),
  fviz_silhouette(SG_clust),
  ncol=2)

grid.arrange(
  fviz_cluster(SF_clust, 
               data = scaled_SF, 
               elipse.type = 'convex', 
               main='Kmeans SF'),
  fviz_silhouette(SF_clust),
  ncol=2)

grid.arrange(
  fviz_cluster(PF_clust, 
               data = scaled_PF, 
               elipse.type = 'convex', 
               main='Kmeans PF'),
  fviz_silhouette(PF_clust),
  ncol=2)

grid.arrange(
  fviz_cluster(C_clust, 
               data = scaled_C, 
               elipse.type = 'convex', 
               main='Kmeans C'),
  fviz_silhouette(C_clust),
  ncol=2)
```

### PAM clustering.

```{r}
PG_clust_pam <- eclust(scaled_PG, FUNcluster = 'pam', k=2, graph=F)

SG_clust_pam <- eclust(scaled_SG, FUNcluster = 'pam', k=2, graph=F)

SF_clust_pam <- eclust(scaled_SF, FUNcluster = 'pam', k=2, graph=F)

PF_clust_pam <- eclust(scaled_PF, FUNcluster = 'pam', k=2, graph=F)

C_clust_pam <- eclust(scaled_C, FUNcluster = 'pam', k=2, graph=F)
```

```{r}
grid.arrange(
  fviz_cluster(PG_clust_pam, 
               data = scaled_PG, 
               elipse.type = 'convex', 
               main='PAM PG'),
  fviz_silhouette(PG_clust_pam),
  ncol=2)

grid.arrange(
  fviz_cluster(SG_clust_pam, 
               data = scaled_SG, 
               elipse.type = 'convex', 
               main='PAM SG'),
  fviz_silhouette(SG_clust_pam),
  ncol=2)

grid.arrange(
  fviz_cluster(SF_clust_pam, 
               data = scaled_SF, 
               elipse.type = 'convex', 
               main='PAM SF'),
  fviz_silhouette(SF_clust_pam),
  ncol=2)

grid.arrange(
  fviz_cluster(PF_clust_pam, 
               data = scaled_PF, 
               elipse.type = 'convex', 
               main='PAM PF'),
  fviz_silhouette(PF_clust_pam),
  ncol=2)

grid.arrange(
  fviz_cluster(C_clust_pam, 
               data = scaled_C, 
               elipse.type = 'convex', 
               main='PAM C'),
  fviz_silhouette(C_clust_pam),
  ncol=2)
```

### Which method is better for further analysis?

::: {style="text-align: justify"}
To further interpret the dependencies and relationships within the clusters one method should be chosen. In order to decide whether K-means or PAM clustering works better in this case, the weighted average will be calculated, treating the number of players in each group, based on their court role (number of observations), as weights for the average silhouette value within such group.
:::

```{r}
kmeans_sils = c(PG_clust$silinfo$avg.width, SG_clust$silinfo$avg.width, SF_clust$silinfo$avg.width, PF_clust$silinfo$avg.width, C_clust$silinfo$avg.width)

kmeans_weigths = c(length(stats_PG[,1]), length(stats_SG[,1]),length(stats_SF[,1]), length(stats_PF[,1]), length(stats_C[,1]))

weighted.mean(kmeans_sils, kmeans_weigths)

pam_sils = c(PG_clust_pam$silinfo$avg.width, SG_clust_pam$silinfo$avg.width, SF_clust_pam$silinfo$avg.width, PF_clust_pam$silinfo$avg.width, C_clust_pam$silinfo$avg.width)

pam_weigths = c(length(stats_PG[,1]), length(stats_SG[,1]),length(stats_SF[,1]), length(stats_PF[,1]), length(stats_C[,1]))

weighted.mean(pam_sils, pam_weigths)
```

::: {style="text-align: justify"}
Average width for the whole data set based on court role is minimally higher for the PAM clustering method.
:::

## Hierarchical clustering.

### Optimal number of clusters.

::: {style="text-align: justify"}
We consider two types of hierarchical clustering, divisive (top-down) and agglomerative (bottom-up). The divisive approach means that division starts from each point being in the same cluster, dividing them then into smaller pairs, with the agglomerative approach being the opposite of the aforementioned.
:::

```{r}
fviz_nbclust(scaled_PG, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical PG')

fviz_nbclust(scaled_SG, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical SG')

fviz_nbclust(scaled_SF, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical SF')

fviz_nbclust(scaled_PF, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical PF')

fviz_nbclust(scaled_C, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical C')
```

::: {style="text-align: justify"}
For each on-court position, as in the partition clustering, the best number of clusters is 2. The analysis with the both top-down and bottom-up approach will be conducted, taking various linkage types into consideration. The best method will be selected based on the silhouette width, as in the case of partition clustering and the values will be compared to select the result, on which the final analysis of clusters contents will be conducted.
:::

```{r}
# setting names of rows to Players Names to better visualize dendrograms

rownames(scaled_PG) = stats_PG$Player.Name
rownames(scaled_SG) = stats_SG$Player.Name
rownames(scaled_SF) = stats_SF$Player.Name
rownames(scaled_PF) = stats_PF$Player.Name
rownames(scaled_C) = stats_C$Player.Name
```

### Divisive clustering - DIANA, Ward linkage.

```{r}
library(fpc)
PG_clust_diana <- eclust(scaled_PG, k=2, FUNcluster = 'diana', hc_method = 'ward.D')
pltree(PG_clust_diana, hang = -1, main = "PG Dendrogram - DIANA", cex = 0.47)
rect.hclust(PG_clust_diana, k=2, border='red')

diana_stats_PG <- cluster.stats(dist(scaled_PG), PG_clust_diana$cluster)
diana_stats_PG$avg.silwidth
diana_stats_PG$cluster.size
```

```{r}
SG_clust_diana <- eclust(scaled_SG, k=2, FUNcluster = 'diana', hc_method = 'ward.D')
pltree(SG_clust_diana, hang = -1, main = "SG Dendrogram - DIANA", cex = 0.47)
rect.hclust(SG_clust_diana, k=2, border='red')

diana_stats_SG <- cluster.stats(dist(scaled_SG), SG_clust_diana$cluster)
diana_stats_SG$avg.silwidth
diana_stats_SG$cluster.size
```

```{r}
SF_clust_diana <- eclust(scaled_SF, k=2, FUNcluster = 'diana', hc_method = 'ward.D')
pltree(SF_clust_diana, hang = -1, main = "SF Dendrogram - DIANA", cex = 0.47)
rect.hclust(SF_clust_diana, k=2, border='red')

diana_stats_SF <- cluster.stats(dist(scaled_SF), SF_clust_diana$cluster)
diana_stats_SF$avg.silwidth
diana_stats_SF$cluster.size
```

```{r}
PF_clust_diana <- eclust(scaled_PF, k=2, FUNcluster = 'diana', hc_method = 'ward.D')
pltree(PF_clust_diana, hang = -1, main = "PF Dendrogram - DIANA", cex = 0.47)
rect.hclust(PF_clust_diana, k=2, border='red')

diana_stats_PF <- cluster.stats(dist(scaled_PF), PF_clust_diana$cluster)
diana_stats_PF$avg.silwidth
diana_stats_PF$cluster.size
```

```{r}
C_clust_diana <- eclust(scaled_C, k=2, FUNcluster = 'diana', hc_method = 'ward.D')
pltree(C_clust_diana, hang = -1, main = "C Dendrogram - DIANA", cex = 0.47)
rect.hclust(C_clust_diana, k=2, border='red')

diana_stats_C <- cluster.stats(dist(scaled_C), C_clust_diana$cluster)
diana_stats_C$avg.silwidth
diana_stats_C$cluster.size
```

### Agglomerative clustering - AGNES, Ward linkage.

```{r}
PG_clust_agnes <- eclust(scaled_PG, k=2, FUNcluster = 'agnes', hc_method = 'ward.D')
pltree(PG_clust_agnes, hang = -1, main = "PG Dendrogram - AGNES", cex = 0.47)
rect.hclust(PG_clust_agnes, k=2, border='red')

agnes_stats_PG <- cluster.stats(dist(scaled_PG), PG_clust_agnes$cluster)
agnes_stats_PG$avg.silwidth
agnes_stats_PG$cluster.size
```

```{r}
SG_clust_agnes <- eclust(scaled_SG, k=2, FUNcluster = 'agnes', hc_method = 'ward.D')
pltree(SG_clust_agnes, hang = -1, main = "SG Dendrogram - AGNES", cex = 0.47)
rect.hclust(SG_clust_agnes, k=2, border='red')

agnes_stats_SG <- cluster.stats(dist(scaled_SG), SG_clust_agnes$cluster)
agnes_stats_SG$avg.silwidth
agnes_stats_SG$cluster.size
```

```{r}
SF_clust_agnes <- eclust(scaled_SF, k=2, FUNcluster = 'agnes', hc_method = 'ward.D')
pltree(SF_clust_agnes, hang = -1, main = "SF Dendrogram - AGNES", cex = 0.47)
rect.hclust(SF_clust_agnes, k=2, border='red')

agnes_stats_SF <- cluster.stats(dist(scaled_SF), SF_clust_agnes$cluster)
agnes_stats_SF$avg.silwidth
agnes_stats_SF$cluster.size
```

```{r}
PF_clust_agnes <- eclust(scaled_PF, k=2, FUNcluster = 'agnes', hc_method = 'ward.D')
pltree(PF_clust_agnes, hang = -1, main = "PF Dendrogram - AGNES", cex = 0.47)
rect.hclust(PF_clust_agnes, k=2, border='red')

agnes_stats_PF <- cluster.stats(dist(scaled_PF), PF_clust_agnes$cluster)
agnes_stats_PF$avg.silwidth
agnes_stats_PF$cluster.size
```

```{r}
C_clust_agnes <- eclust(scaled_C, k=2, FUNcluster = 'agnes', hc_method = 'ward.D')
pltree(C_clust_agnes, hang = -1, main = "C Dendrogram - AGNES", cex = 0.47)
rect.hclust(C_clust_agnes, k=2, border='red')

agnes_stats_C <- cluster.stats(dist(scaled_C), C_clust_agnes$cluster)
agnes_stats_C$avg.silwidth
agnes_stats_C$cluster.size
```

::: {style="text-align: justify"}
The results show that divisive clustering algorithm got a bit 'lost' during division of the data to clusters. There are mostly clusters which contain almost every player from the data-set in one cluster and just a few in the other cluster. Such results do not give us any insights into the data.

On the other hand the result for agglomerative approach are a bit closer to the expected outcome, with the contents of clusters representing quite visible division into "positive/negative impact" players, nevertheless the silhouette width values are lower than for the PAM algorithm, therefore the final analysis will be conducted on the results of the PAM clustering.
:::

## Data analysis within clusters.

```{r}
stats_PG['cluster_label'] = PG_clust_pam$clustering
stats_SG['cluster_label'] = SG_clust_pam$clustering
stats_SF['cluster_label'] = SF_clust_pam$clustering
stats_PF['cluster_label'] = PF_clust_pam$clustering
stats_C['cluster_label'] = C_clust_pam$clustering
```

### Comparison of statistics inside of clusters.

```{r}
summary(stats_PG[stats_PG$cluster_label == 1,2:12])
summary(stats_PG[stats_PG$cluster_label == 2,2:12])
```

```{r}
summary(stats_SG[stats_SG$cluster_label == 1,2:12])
summary(stats_SG[stats_SG$cluster_label == 2,2:12])
```

```{r}
summary(stats_SF[stats_SF$cluster_label == 1,2:12])
summary(stats_SF[stats_SF$cluster_label == 2,2:12])
```

```{r}
summary(stats_PF[stats_PF$cluster_label == 1,2:12])
summary(stats_PF[stats_PF$cluster_label == 2,2:12])
```

```{r}
summary(stats_C[stats_C$cluster_label == 1,2:12])
summary(stats_C[stats_C$cluster_label == 2,2:12])
```

### Conclusions.

::: {style="text-align: justify"}
Comparison of the statistics within the clusters quite well reflects the differences between the players abilities and impact. The conclusion is that each time one of the clusters gathers players with significant impact on the court and the other one gathers players which have some areas that need improvements. It is especially visible by the statistics of VORP and WS.48, which in the case of cluster containing better players are way above league average (shown in the section of 'Data set review'), with the other one below the league standard. The interpretation is quite simple - the players in that group significantly contribute to the teams winning and their replacement is most likely to cause disadvantage on the court or the team.

To give a few more examples, below, the reader may find Top 6 players for each position out of both clusters. As also the silhouette graphs suggest, there are some players, that are quite on the edge of both clusters, which is visible also on the surnames display, but as the boundary has to be drawn somewhere, it is important to always look carefully at the results of algorithms' work.
:::

```{r}
cat('Point Guards\n')
head(stats_PG[stats_PG$cluster_label == 1,1])
head(stats_PG[stats_PG$cluster_label == 2,1])

cat('Shooting Guards\n')
head(stats_SG[stats_SG$cluster_label == 1,1])
head(stats_SG[stats_SG$cluster_label == 2,1])

cat('Small Forwards\n')
head(stats_SF[stats_SF$cluster_label == 1,1])
head(stats_SF[stats_SF$cluster_label == 2,1])

cat('Power Forwards\n')
head(stats_PF[stats_PF$cluster_label == 1,1])
head(stats_PF[stats_PF$cluster_label == 2,1])

cat('Centers\n')
head(stats_C[stats_C$cluster_label == 1,1])
head(stats_C[stats_C$cluster_label == 2,1])
```

## References.

::: {style="text-align: justify"}
<https://bestballstats.com/2022/07/23/generating-nba-archetypes-using-k-means-clustering/>

<https://towardsdatascience.com/redefining-nba-player-classifications-using-clustering-36a348fa54a8>

<https://www.basketball-reference.com/about/glossary.html>
:::
