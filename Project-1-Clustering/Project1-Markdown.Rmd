---
title: "Project1-Markdown"
output: html_document
date: "2023-12-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# NBA Salaries & Statistics data set - Clustering

## Introduction.

::: {style="text-align: justify"}
The NBA basketball league is famous for its players unique level of skill. Those players, who are good enough to play in the league are selected carefully by the scouts, with the statistics of their performance being constantly analysed.

The goal of the project is to get as many valuable insights into the data set as possible with the clustering technique, which might help with assessing players on-court value. Although clustering, as a part of the unsupervised learning field, is used to identify the possible groups in the data in instances where labels for such groups are not provided, and in case of this data set there in fact is some form of labeling (court roles are assigned), the aim is to provide the information based on the value which players bring to the court rather than simple assignment of the on-court position.

I would like to conduct an analysis which might divide players into clusters based on their statistics inside of the on-court role groups. I would also like to check if some players will fall out of the clusters and if so to also try to find an answer why.
:::

## Review of existing solutions.

::: {style="text-align: justify"}
While doing the research of currently existing solutions for this project I came across some interesting analyses, but they are mostly focused on obtaining hidden skills and tendencies of players.

For example, the author of "Generating NBA Archetypes Using K-MeansÂ Clustering" article (<https://bestballstats.com/2022/07/23/generating-nba-archetypes-using-k-means-clustering/>) is separating the defensive and offensive roles on the court and assigning the archetypes in each of those categories with no regards to the classical on-court positioning, focusing only on the statistics which define the tendencies on the court. The archetypes themselves are created based on k-means clustering by the chosen statistics and named after analysis of the clusters contents. Such approach might be helpful when trying to find a player with a certain play-style on the given side of the court.

In another interesting article (<https://towardsdatascience.com/redefining-nba-player-classifications-using-clustering-36a348fa54a8>) the author uses the PCA dimension reduction technique to gather information about the players from the wide range of the statistics. After the reduction, author describes "ingredients" that contribute the most to creation of every Principal Components and which on court tendencies each PC reflects. On top of that hierarchical clustering is conducted, grouping the players based on their profiles, which are developed based on the PC "ingredients".

Taking into consideration the focus of above analyses on players tendencies and on-court role archetypes I believe that there exists some space for conducting the analysis focusing on the on-court impact itself and that combining the results of archetype-focused analysis with impact-focused analysis might bring valuable insights into teams rosters.
:::

## Data set review and preparation.

```{r}
data <- read.csv('nba_2022-23_all_stats_with_salary.csv')

```

```{r}
data$Position = as.factor(data$Position)
summary(data$Position)
data[data$Position == 'PG-SG' | data$Position == 'SF-PF' | data$Position == 'SF-SG' | data$Position == 'SG-PG',]
```

::: {style="text-align: justify"}
There are 7 players that are assigned with a mix of a classically considered basketball court positions. In order to achieve coherent position labeling, I have decided to manually assign positions (PG, SG, SF, PF, C) to those players taking into consideration either their historical roles or their role in lineups for the season.
:::

```{r}
data$Position = as.character(data$Position)

# Kyrie Irving - PG
data[data$Player.Name == 'Kyrie Irving',]['Position'] = 'PG'
# Mikal Bridges
data[data$Player.Name == 'Mikal Bridges',]['Position'] = 'SF'
# Spencer Dinwiddie
data[data$Player.Name == 'Spencer Dinwiddie',]['Position'] = 'PG'
# Patrick Beverley
data[data$Player.Name == 'Patrick Beverley',]['Position'] = 'PG'
# Matisse Thybulle
data[data$Player.Name == 'Matisse Thybulle',]['Position'] = 'SF'
# George Hill
data[data$Player.Name == 'George Hill',]['Position'] = 'PG'
# Kevin Knox
data[data$Player.Name == 'Kevin Knox',]['Position'] = 'SF'

data$Position = as.factor(data$Position)
summary(data$Position)
```

::: {style="text-align: justify"}
Next step is to select statistics, based on which the clustering will be conducted. Chosen stats quite well reflect players influence on the team performance. There is only one row which has 'NA' values, therefore it is removed from the data set - the player did not attempt any shots during his time on court. Furthermore during analysis of the data set it turned out, that Nikola Jokic, Finals MVP from season 2022-2023, is not included, so he was added manually, as player with such significant impact on the team would for sure stand as a good reference point for further analysis.
:::

```{r}
shorter_stats <- data[c('Player.Name', 'Position', 'MP', 'PTS', 'AST', 'STL', 'BLK', 'TRB', 'eFG.', 'TS.', 'USG.', 'WS.48','VORP')]

shorter_stats[!complete.cases(shorter_stats),]

shorter_stats = shorter_stats[complete.cases(shorter_stats),]

nikola_jokic = c('Nikola Jokic', 'C', 33.7, 24.5, 9.8, 1.3, 0.7, 11.8, 0.66, 0.701, 27.2, 0.308, 8.8)

shorter_stats <- rbind(shorter_stats, nikola_jokic)

numeric_cols <- c('MP', 'PTS', 'AST', 'STL', 'BLK', 'TRB', 'eFG.', 'TS.', 'USG.', 'WS.48','VORP')

shorter_stats[numeric_cols] <- lapply(shorter_stats[numeric_cols], as.numeric)

columns_shortcut = c('Player.Name','MP', 'PTS', 'AST', 'STL', 'BLK', 'TRB', 'eFG.', 'TS.', 'USG.', 'WS.48','VORP')
```

::: {style="text-align: justify"}
The data is divided to 5 sub-groups based on the on-court role. Statistical summary of characteristics for each group may be found below.
:::

```{r}

stats_PG = shorter_stats[shorter_stats$Position == 'PG', columns_shortcut]
summary(stats_PG[,2:12])
```

```{r echo=TRUE}

stats_SG = shorter_stats[shorter_stats$Position == 'SG', columns_shortcut]
summary(stats_SG[,2:12])
```

```{r}
stats_SF = shorter_stats[shorter_stats$Position == 'SF', columns_shortcut]
summary(stats_SF[,2:12])
```

```{r}
stats_PF = shorter_stats[shorter_stats$Position == 'PF', columns_shortcut]
summary(stats_PF[,2:12])

```

```{r}
stats_C = shorter_stats[shorter_stats$Position == 'C', columns_shortcut]
summary(stats_C[,2:12])
```

::: {style="text-align: justify"}
Correlation matrices for statistics from each group of court roles show positive relations between the statistics, therefore, knowing that they are used to measure players impact on the field one can assume that chosen statistics would reflect players positive or negative contribution towards teams' performance.
:::

```{r}
library(gridExtra)
library(corrplot)

corrplot(cor(stats_PG[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats PG", mar=c(0,0,2,0))

corrplot(cor(stats_SG[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats SG", mar=c(0,0,2,0))

corrplot(cor(stats_SF[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats SF", mar=c(0,0,2,0))

corrplot(cor(stats_PF[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats PF", mar=c(0,0,2,0))

corrplot(cor(stats_C[,2:12], use="complete"), method="number", type="upper", diag=F, tl.col="black", tl.srt=45, tl.cex=0.9, number.cex=0.8, title="Stats C", mar=c(0,0,2,0))
```

The data is then standardized with the z-score method.

```{r}
scaled_PG = as.data.frame(lapply(stats_PG[,2:12], scale))
scaled_SG = as.data.frame(lapply(stats_SG[,2:12], scale))
scaled_SF = as.data.frame(lapply(stats_SF[,2:12], scale))
scaled_PF = as.data.frame(lapply(stats_PF[,2:12], scale))
scaled_C = as.data.frame(lapply(stats_C[,2:12], scale))
```

## Clustering tendency.

::: {style="text-align: justify"}
The data clustering tendency is checked for each court-role. The Hopkins stat is either close to 0.7 or above for each group, therefore significant clusters might exist.
:::

```{r}
library(factoextra)
library(cluster)

get_clust_tendency(scaled_PG, 2, graph=T)
get_clust_tendency(scaled_SG, 2, graph=T)
get_clust_tendency(scaled_SF, 2, graph=T)
get_clust_tendency(scaled_PF, 2, graph=T)
get_clust_tendency(scaled_C, 2, graph=T)
```

## Partitional clustering.

### Optimal number of clusters for each group based on court role.

::: {style="text-align: justify"}
Knowing that the data could be significantly clustered we continue the analysis with checking how well the data could be clustered with the k-means and PAM clustering methods and what is more important for how many clusters it would work the best.

The key difference between these two methods is that k-means is based on idea of centroids, while PAM is based on the idea of medoids.

The CLARA method has no use in this case, as the data set is relatively small - PAM algorithm should be good enough (as they both work based on medoids concept).
:::

```{r}
PG_viz_kmeans = fviz_nbclust(scaled_PG, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans PG')

SG_viz_kmeans = fviz_nbclust(scaled_SG, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans SG')

SF_viz_kmeans = fviz_nbclust(scaled_SF, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans SF')

PF_viz_kmeans = fviz_nbclust(scaled_PF, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans PF')

C_viz_kmeans = fviz_nbclust(scaled_C, FUNcluster = kmeans, method = 'silhouette') + theme_classic() + ggtitle('Kmeans C')
```

```{r}
PG_viz_pam = fviz_nbclust(scaled_PG, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM PG')

SG_viz_pam = fviz_nbclust(scaled_SG, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM SG')

SF_viz_pam = fviz_nbclust(scaled_SF, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM SF')

PF_viz_pam = fviz_nbclust(scaled_PF, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM PF')

C_viz_pam = fviz_nbclust(scaled_C, FUNcluster = cluster::pam, method = 'silhouette') + theme_classic() + ggtitle('PAM C')
```

```{r}
library(gridExtra)

grid.arrange(PG_viz_kmeans, PG_viz_pam, ncol=2)
grid.arrange(SG_viz_kmeans, SG_viz_pam, ncol=2)
grid.arrange(SF_viz_kmeans, SF_viz_pam, ncol=2)
grid.arrange(PF_viz_kmeans, PF_viz_pam, ncol=2)
grid.arrange(C_viz_kmeans, C_viz_pam, ncol=2)
```

::: {style="text-align: justify"}
The results are consistent in terms of number of clusters - the best silhouette values are returned when 2 clusters are selected.
:::

### K-means clustering.

```{r}
PG_clust <- eclust(scaled_PG, FUNcluster = 'kmeans', k=2, graph=F)

SG_clust <- eclust(scaled_SG, FUNcluster = 'kmeans', k=2, graph=F)

SF_clust <- eclust(scaled_SF, FUNcluster = 'kmeans', k=2, graph=F)

PF_clust <- eclust(scaled_PF, FUNcluster = 'kmeans', k=2, graph=F)

C_clust <- eclust(scaled_C, FUNcluster = 'kmeans', k=2, graph=F)
```

```{r}
grid.arrange(
  fviz_cluster(PG_clust, 
               data = scaled_PG, 
               elipse.type = 'convex', 
               main='Kmeans PG'),
  fviz_silhouette(PG_clust),
  ncol=2)

grid.arrange(
  fviz_cluster(SG_clust, 
               data = scaled_SG, 
               elipse.type = 'convex', 
               main='Kmeans SG'),
  fviz_silhouette(SG_clust),
  ncol=2)

grid.arrange(
  fviz_cluster(SF_clust, 
               data = scaled_SF, 
               elipse.type = 'convex', 
               main='Kmeans SF'),
  fviz_silhouette(SF_clust),
  ncol=2)

grid.arrange(
  fviz_cluster(PF_clust, 
               data = scaled_PF, 
               elipse.type = 'convex', 
               main='Kmeans PF'),
  fviz_silhouette(PF_clust),
  ncol=2)

grid.arrange(
  fviz_cluster(C_clust, 
               data = scaled_C, 
               elipse.type = 'convex', 
               main='Kmeans C'),
  fviz_silhouette(C_clust),
  ncol=2)
```

### PAM clustering.

```{r}
PG_clust_pam <- eclust(scaled_PG, FUNcluster = 'pam', k=2, graph=F)

SG_clust_pam <- eclust(scaled_SG, FUNcluster = 'pam', k=2, graph=F)

SF_clust_pam <- eclust(scaled_SF, FUNcluster = 'pam', k=2, graph=F)

PF_clust_pam <- eclust(scaled_PF, FUNcluster = 'pam', k=2, graph=F)

C_clust_pam <- eclust(scaled_C, FUNcluster = 'pam', k=2, graph=F)
```

```{r}
grid.arrange(
  fviz_cluster(PG_clust_pam, 
               data = scaled_PG, 
               elipse.type = 'convex', 
               main='PAM PG'),
  fviz_silhouette(PG_clust_pam),
  ncol=2)

grid.arrange(
  fviz_cluster(SG_clust_pam, 
               data = scaled_SG, 
               elipse.type = 'convex', 
               main='PAM SG'),
  fviz_silhouette(SG_clust_pam),
  ncol=2)

grid.arrange(
  fviz_cluster(SF_clust_pam, 
               data = scaled_SF, 
               elipse.type = 'convex', 
               main='PAM SF'),
  fviz_silhouette(SF_clust_pam),
  ncol=2)

grid.arrange(
  fviz_cluster(PF_clust_pam, 
               data = scaled_PF, 
               elipse.type = 'convex', 
               main='PAM PF'),
  fviz_silhouette(PF_clust_pam),
  ncol=2)

grid.arrange(
  fviz_cluster(C_clust_pam, 
               data = scaled_C, 
               elipse.type = 'convex', 
               main='PAM C'),
  fviz_silhouette(C_clust_pam),
  ncol=2)
```

### Which method is better for further analysis?

::: {style="text-align: justify"}
To further interpret the dependencies and relationships within the clusters one method should be chosen. In order to decide whether K-means or PAM clustering works better in this case, the weighted average will be calculated, treating the number of players in each group, based on their court role (number of observations), as weights for the average silhouette value within such group.
:::

```{r}
kmeans_sils = c(PG_clust$silinfo$avg.width, SG_clust$silinfo$avg.width, SF_clust$silinfo$avg.width, PF_clust$silinfo$avg.width, C_clust$silinfo$avg.width)

kmeans_weigths = c(length(stats_PG[,1]), length(stats_SG[,1]),length(stats_SF[,1]), length(stats_PF[,1]), length(stats_C[,1]))

weighted.mean(kmeans_sils, kmeans_weigths)

pam_sils = c(PG_clust_pam$silinfo$avg.width, SG_clust_pam$silinfo$avg.width, SF_clust_pam$silinfo$avg.width, PF_clust_pam$silinfo$avg.width, C_clust_pam$silinfo$avg.width)

pam_weigths = c(length(stats_PG[,1]), length(stats_SG[,1]),length(stats_SF[,1]), length(stats_PF[,1]), length(stats_C[,1]))

weighted.mean(pam_sils, pam_weigths)
```

::: {style="text-align: justify"}
Average width for the whole data set based on court role is higher for the PAM clustering method.
:::

## Hierarchical clustering.

We consider two types of hierarchical clustering, divisive (top-down) and agglomerative (bottom-up). The divisive approach means that division starts from each point being in the same cluster, divising them then in pairs into smaller ones, with the agglomerative approach being the opposite of the aforementioned.

```{r}
fviz_nbclust(scaled_PG, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical PG')

fviz_nbclust(scaled_SG, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical SG')

fviz_nbclust(scaled_SF, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical SF')

fviz_nbclust(scaled_PF, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical PF')

fviz_nbclust(scaled_C, FUNcluster = hcut, method = 'silhouette') + theme_classic() + ggtitle('Hierarchical C')
```

For each on-court position, as in the partition clustering, the best number of clusters is 2. The analysis with the both top-down and bottom-up approach will be conducted, taking various linkage types into consideration. The best method will be selected based on the silhouette width, as in the case of partition clustering and the values will be compared to select the result, on which the final analysis of clusters contents will be conducted.

```{r}

```

## Data analysis within clusters.
